\documentclass{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{amsmath,amssymb,amsfonts,amsthm}

\begin{document}
\title{J. R. "Bob" Dobbs Memorial Inverse Hessian-Vector Multiplication Method}
\author{Howon Lee}
\date{2019 January}
\maketitle

\begin{quote}
I'm not saying that machine learning is the portal to a demon universe, I'm just saying that some doors are best left unopened. - James Mickens
\end{quote}

\section{Abstract}
You care about multiplication of the inverse Hessian with a vector because Newton's method for optimization demands it. In many applications, inverse Hessian multiplication is not possible because of time and space constraints. A method linear to the size of the gradient function in both time and space is given which is very close to the Hessian-vector multiplication of Pearlmutter (1993), with differential operator and all. However, I have failed to get it working on neural nets for some reasons also given. I also have some speculations on what second order methods mean to the enterprise of machine learning, pilfered from physics and economics.

\section{Introduction}

The Hessian is the matrix of second derivatives of a function. In Newton's method for optimizations, it is the second derivative with respect to an optimization target. But Newton's method requires the inverse of the Hessian (in one dimension, it requires the inverse of the second derivative). This led to Newton's method being computationally infeasible for most optimizational regimes where the dimensionality is high, because if the dimension of the system is $n$, the naive inversion of the Newton's method takes $O(n^2)$ space and $O(n^3)$ time.

There is a shortcut which exists which is usually more used in neural networks. In neural networks it was introduced by Pearlmutter 1993. This consists in noting that the Hessian is a Jacobian itself and finding the Hessian in the Taylor approximation of the gradient about a point. This means that the Hessian-vector multiplication can be found like a sparse matrix-vector multiplication, linear to the order of the vector. However this gives a Hessian-vector multiplication only, not an inverse Hessian-vector multiplication, so one must use Krylov methods to get optimization done. In the following, an inverse Hessian-vector multiplication is shown.

\section{J. R. "Bob" Dobbs Method}

Recall the Hessian is a Jacobian, and it is found in the Taylor expansion of the gradient of a function about a point. The crux of the J. R. "Bob" Dobbs method is to realize that the inverse Hessian is itself a Jacobian and it is found in the Taylor expansion of the functional inverse of the gradient of a function about a point.

\subsection{Inverse Hessian is a Jacobian}

The inverse function theorem (Lang 1995) gives a formula for the matrix inverse of a Jacobian, with conditions (Jacobian determinant must be nonzero). That is, the Jacobian of the (functional) inverse of a function is the multiplicative inverse of the Jacobian of the original function.

Hessian is Jacobian of the gradient:

$$H(f(x)) = J(\nabla f(x))^T $$

So, the inverse Hessian is Jacobian of the functional inverse of the gradient:

$$H^{-1}(f(x)) = J(\nabla^{-1} f(x))^T $$

\subsection{Inverse Hessian is found in the Taylor Expansion of $\nabla_w^{-1}$}

Pearlmutter's 1993 method for Hessian-vector multiplication begins from the Taylor expansion of the gradient about a point:

%%% expansion

But, given our definition of $H^{-1}$ above, we will actually find $H^{-1}$ about the expansion of the functional inverse of the gradient about a point.

%%% expansion

After finding this, the entire rest of the procedure goes analogously to Pearlmutter 1993.

%%%% rest of procedure

\section{Examples}

$$ f_1(x_i)_i = x_i^3 $$

%%%%%

$$ f_2(x_i)_i = (x -y) ^ 2 $$
%%%%%

\section{Neural Net}

%%%%% document failure

\section{Discussion}

%%%% discuss
\section{Citations}

Lang 1995 Differential and Riemannian Manifolds
Pearlmutter 1993

%%%%% citations

\end{document}
