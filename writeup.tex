\documentclass{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{amsmath,amssymb,amsfonts,amsthm}

\begin{document}
\title{J. R. "Bob" Dobbs Memorial Inverse Hessian-Vector Multiplication Method}
\author{Howon Lee}
\date{2019 January}
\maketitle

\begin{quote}
I'm not saying that machine learning is the portal to a demon universe, I'm just saying that some doors are best left unopened. - James Mickens
\end{quote}

\section{Abstract}
You care about multiplication of the inverse Hessian with a vector because Newton's method for optimization demands it. In many applications, inverse Hessian multiplication is not possible because of time and space constraints. A method linear to the size of the gradient function in both time and space is given which is very close to the Hessian-vector multiplication of Pearlmutter (1993), with differential operator and all. However, I have failed to get it working on neural nets for a reason also given. I also have some speculations on what second order methods mean to the enterprise of machine learning, pilfered from physics and economics.

\section{Introduction}

The Hessian is the matrix of second derivatives of a function. In Newton's method for optimizations, it is the second derivative with respect to an optimization target. But Newton's method requires the inverse of the Hessian (in one dimension, it requires the inverse of the second derivative). This led to Newton's method being computationally infeasible for most optimizational regimes where the dimensionality is high, because if the dimension of the system is $n$, the naive inversion of the Newton's method takes $O(n^2)$ space and $O(n^3)$ time.

There is a shortcut which exists which is usually more used in neural networks. In neural networks it was introduced by Pearlmutter 1993. This consists in noting that the Hessian is a Jacobian itself and finding the Hessian in the Taylor approximation of the gradient about a point. This means that the Hessian-vector multiplication can be found like a sparse matrix-vector multiplication, linear to the order of the vector. However this gives a Hessian-vector multiplication only, not an inverse Hessian-vector multiplication, so one must use Krylov methods to get optimization done. In the following, an inverse Hessian-vector multiplication is shown.

\section{J. R. "Bob" Dobbs Method}

Recall the Hessian is a Jacobian, and it is found in the Taylor expansion of the gradient of a function about a point. The crux of the J. R. "Bob" Dobbs method is to realize that the inverse Hessian is itself a Jacobian and it is found in the Taylor expansion of the functional inverse of the gradient of a function about a point.

\subsection{Inverse Hessian is a Jacobian}

The inverse function theorem (Lang 1995) gives a formula for the matrix inverse of a Jacobian, with conditions (Jacobian determinant must be nonzero). That is, the Jacobian of the (functional) inverse of a function is the multiplicative inverse of the Jacobian of the original function.

Hessian is Jacobian of the gradient:

$$H(f(x)) = J(\nabla f(x))^T $$

So, the inverse Hessian is Jacobian of the functional inverse of the gradient:

$$H^{-1}(f(x)) = J(\nabla^{-1} f(x))^T $$

\subsection{Inverse Hessian is found in the Taylor Expansion of $\nabla_w^{-1}$}

Pearlmutter's 1993 method for Hessian-vector multiplication begins from the Taylor expansion of the gradient about a point:

%%% expansion

But, given our definition of $H^{-1}$ above, we will actually find $H^{-1}$ about the expansion of the functional inverse of the gradient about a point.

%%% expansion

After finding this, the entire rest of the procedure goes analogously to Pearlmutter 1993.

%%%% rest of procedure

\section{Examples}

$$ f_1(x_i)_i = x_i^3 $$

%%%%%

$$ f_2(x_i)_i = (x -y) ^ 2 $$
%%%%% linear op, actually

\section{Neural Net}

Of particular interest in application of Newton's method to high dimensional systems is in neural networks. I have made several attempts to attack them with this method but it is surprisingly difficult to get the functional inverse of a neural network gradient with respect to the weights which is compatible with the differential operator.

With minibatches (but only with very large minibatches), the functional inverse of the gradient is actually pretty trivial. However, with actual application of the differential operator, I found that unlike the DAG structure of the original Pearlmutter application of the operator, initial results require further results. Of course one could just approximate those numerically, so at least I present what I have right now for a 1-hidden-layer fully-connected multilayer perceptron.

You might think that you might use the $R_v$ on the original gradient functions. However, the $v$'s seem to me to be different. I assume you are completely familiar with neural nets already.

%%%%% document failure

\section{Discussion}

%%%% discuss

%%% There are many heterodox views of neural networks, Werbos-Rumelhart-style backpropagation nets. I collect them, and have a few of my own lying around on a back burner somewhere. An important one to me is to think of them as sort of simulations of firms. Another is to think of them as spins on a 1-lattice with weirdo boundary conditions, each layer one spin on that small 1-lattice. Note that this is different from strictly going on a Hopfield net view, because of the semantics of what the spin sites are and the boundary conditions.

%% Second order phenomena are obviously of great interest to both heterodox views. In the firm-simulation it seems to be a firm without intrafirm conflict, sort of a Communist utopian firm, because the taking into account curvature of each parameter with each other parameter seems to coincide with a sort of agreement. This amuses me greatly in that it takes the term "trust region" much more literally than imagined (the contention that machine learning is about credit assignment systems is also taken much more literally than imagined). In the view as 1-lattice with spins, there is a renormalization phenomenon futzing with the exponential correlational structure (the vanishing or exponential gradient) which seems very reminiscent of the onset of criticality in second-order phase transition. But it also seems to me that you need real second-order methods, not Krylov methods, for that renormalization to occur.

%% You might also just care about second-order methods for completely normal optimizational reasons.

\section{Citations}

Lang 1995 Differential and Riemannian Manifolds
Pearlmutter 1993
Werbos 1974

%%%%% citations

\end{document}
